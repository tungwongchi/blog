# Multi role-playing large language model

Posted on 2024-03-16 by Tungwong Chi

## Abstract

Role-playing in conversational AI allows for the simulation of various characters and scenarios, providing rich and diverse interactions. We propose mRP-LLM, a novel approach that integrates multiple role-playing characters into a single model to maximize resource efficiency and expand the model's conversational capabilities.

## 1. Introduction

Role-playing within AI-driven conversational systems offers immersive experiences in various domains. Traditional models require separate instances for each character, leading to high computational costs. We introduce mRP-LLM, a unified model architecture capable of simulating multiple roles, conserving resources, and enabling complex interactions.

## References

- Ma Zhiming. (2024). Roleplay-with-XiYou [Data]. Available at https://github.com/JimmyMa99/Roleplay-with-XiYou
- Shanghai AI Laboratory. (2024). InternLM2-Chat-7B [LLM]. Available at https://modelscope.cn/Shanghai_AI_Laboratory/internlm2-chat-7b
- XTuner Contributors. (2023). XTuner: A Toolkit for Efficiently Fine-tuning LLM [Software]. Available at https://github.com/InternLM/xtuner
- InternLM. (2024). Tutorial. Available at https://github.com/InternLM/Tutorial
- InternLM. (2024). InternLM. Available at https://github.com/InternLM/InternLM

## Appendix

### How to Cite This Article

To reference this article, please use the following formats:

```bibtex
@online{mRP-LLM,
    title={Multi role-playing large language model},
    author={Tungwong Chi},
    year={2024},
    month={03},
    url={\url{https://tungwongchi.github.io/blog_md.html?path=archives/mRP-LLM.md}},
}
```

---

&copy; 2020 Tungwong Chi. All rights reserved.
